using device cpu
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Episodic return of the environment: 0.0
Global step: 4100
Episodic return of the environment: 0.0
Global step: 4200
Global step: 4300
Episodic return of the environment: 0.0
Global step: 4400
Global step: 4500
Episodic return of the environment: 0.0
Global step: 4600
Global step: 4700
Episodic return of the environment: 0.0
Global step: 4800
Global step: 4900
Episodic return of the environment: 0.0
Global step: 5000
Global step: 5100
Episodic return of the environment: 0.0
Global step: 5200
Global step: 5300
Episodic return of the environment: 0.0
Global step: 5400
Global step: 5500
Episodic return of the environment: 0.0
Global step: 5600
Global step: 5700
Episodic return of the environment: 0.0
Global step: 5800
Global step: 5900
Episodic return of the environment: 0.0
Global step: 6000
Global step: 6100
Episodic return of the environment: 0.0
Global step: 6200
Global step: 6300
Episodic return of the environment: 0.0
Global step: 6400
Global step: 6500
Episodic return of the environment: 0.0
Global step: 6600
Global step: 6700
Episodic return of the environment: 0.0
Global step: 6800
Global step: 6900
Episodic return of the environment: 0.0
Global step: 7000
Global step: 7100
Episodic return of the environment: 0.0
Global step: 7200
Global step: 7300
Episodic return of the environment: 0.0
Global step: 7400
Global step: 7500
Episodic return of the environment: 0.0
Global step: 7600
Global step: 7700
Episodic return of the environment: 0.0
Global step: 7800
Global step: 7900
Episodic return of the environment: 0.0
Global step: 8000
Global step: 8100
Episodic return of the environment: 0.0
Global step: 8200
Global step: 8300
Episodic return of the environment: 0.0
Global step: 8400
Global step: 8500
Episodic return of the environment: 0.0
Global step: 8600
Global step: 8700
Episodic return of the environment: 0.0
Global step: 8800
Global step: 8900
Episodic return of the environment: 0.0
Global step: 9000
Global step: 9100
Episodic return of the environment: 0.0
Global step: 9200
Global step: 9300
Episodic return of the environment: 0.0
Global step: 9400
Global step: 9500
Episodic return of the environment: 0.0
Global step: 9600
Global step: 9700
Episodic return of the environment: 0.0
Global step: 9800
Global step: 9900
Episodic return of the environment: 0.0
Global step: 10000
Global step: 10100
Episodic return of the environment: 0.0
Global step: 10200
Global step: 10300
Episodic return of the environment: 0.0
Global step: 10400
Global step: 10500
Episodic return of the environment: 0.0
Global step: 10600
Global step: 10700
Episodic return of the environment: 0.0
Global step: 10800
Global step: 10900
Episodic return of the environment: 0.0
Global step: 11000
Global step: 11100
Episodic return of the environment: 0.0
Global step: 11200
Global step: 11300
Episodic return of the environment: 0.0
Global step: 11400
Global step: 11500
Episodic return of the environment: 0.0
Global step: 11600
Global step: 11700
Episodic return of the environment: 0.0
Global step: 11800
Global step: 11900
Episodic return of the environment: 0.0
Global step: 12000
Global step: 12100
Episodic return of the environment: 0.0
Global step: 12200
Global step: 12300
Episodic return of the environment: 0.0
Global step: 12400
Global step: 12500
Episodic return of the environment: 0.0
Global step: 12600
Global step: 12700
Episodic return of the environment: 0.0
Global step: 12800
Global step: 12900
Episodic return of the environment: 0.0
Global step: 13000
Global step: 13100
Episodic return of the environment: 0.0
Global step: 13200
Global step: 13300
Episodic return of the environment: 0.0
Global step: 13400
Global step: 13500
Episodic return of the environment: 0.0
Global step: 13600
Global step: 13700
Episodic return of the environment: 0.0
Global step: 13800
Global step: 13900
Episodic return of the environment: 0.0
Global step: 14000
Global step: 14100
Episodic return of the environment: 0.0
Global step: 14200
Global step: 14300
Episodic return of the environment: 0.0
Global step: 14400
Global step: 14500
Episodic return of the environment: 0.0
Global step: 14600
Global step: 14700
Episodic return of the environment: 0.0
Global step: 14800
Global step: 14900
Episodic return of the environment: 0.0
Global step: 15000
Global step: 15100
Episodic return of the environment: 0.0
Global step: 15200
Global step: 15300
Episodic return of the environment: 0.0
Global step: 15400
Global step: 15500
Episodic return of the environment: 0.0
Global step: 15600
Global step: 15700
Episodic return of the environment: 0.0
Global step: 15800
Global step: 15900
Episodic return of the environment: 0.0
Global step: 16000
Global step: 16100
Episodic return of the environment: 0.0
Global step: 16200
Traceback (most recent call last):
  File "/home/p.le-tolguenec/Documents/contrastive_exploration/src/baselines/ngu.py", line 349, in <module>
    ngu_optimizer.step()
  File "/home/p.le-tolguenec/torch-pa/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/p.le-tolguenec/torch-pa/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/p.le-tolguenec/torch-pa/lib/python3.9/site-packages/torch/optim/adam.py", line 141, in step
    adam(
  File "/home/p.le-tolguenec/torch-pa/lib/python3.9/site-packages/torch/optim/adam.py", line 281, in adam
    func(params,
  File "/home/p.le-tolguenec/torch-pa/lib/python3.9/site-packages/torch/optim/adam.py", line 345, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt