{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpletctj6\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Authenticate with W&B\n",
    "wandb.login(timeout=1024)\n",
    "\n",
    "# Configure project and other parameters if necessary\n",
    "project_name = \"run_away_sac_explore\"\n",
    "entity = \"pletctj6\"\n",
    "\n",
    "# Retrieve the runs from the project\n",
    "api = wandb.Api()\n",
    "runs = api.runs(f\"{entity}/{project_name}\")\n",
    "\n",
    "# Initialize a list to store coverage and shannon entropy data\n",
    "experiments_data = {}\n",
    "nb_max_samples = int(1e6)\n",
    "TOTAL_TIME_STEPS = 2_000_000\n",
    "TOTAL_POINTS = 1_000\n",
    "NUM_ENVS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "* config : config for the run\n",
    "* history : Time evolution of all the data recorded during the run as columns in a pandas dataframe\n",
    "* summary : last sample of the data recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe634a4361a44b6b4b81b7aef96e615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing runs:   0%|          | 0/1442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from envs.config_env import config as config_env\n",
    "\n",
    "def process_run(run, \n",
    "                metrics = {\n",
    "                            \"config\": [\"exp_name\", \"env_id\", \"seed\", \"keep_extrinsic_reward\", \"beta_ratio\", \"name\"],\n",
    "                            \"history\": [\"charts/shannon_entropy\", \"charts/coverage\",  \"_step\"],\n",
    "                            \"summary\": [\"charts/shannon_entropy\", \"charts/coverage\",  \"_step\"]\n",
    "                },\n",
    "                config_env=config_env):\n",
    "    # Vérification de l'état du run\n",
    "    # if run.state != \"finished\":\n",
    "    #     # print(f\"Skipping run {run.name} because it is not finished.\")\n",
    "    #     return None\n",
    "    ##### CONFIGURATION #####\n",
    "    config = run.config\n",
    "    config_metrics = {key: None for key in metrics['config']}\n",
    "    for key in config_metrics.keys():\n",
    "        try:\n",
    "            config_metrics[key] = config.get(key)\n",
    "        except:\n",
    "            print(f\"Skipping run {run.name} because it doesn't have the data {key} in config.\")\n",
    "            return None\n",
    "        \n",
    "    ##### HISTORY #####\n",
    "    history_metrics_full = run.history(samples=nb_max_samples, keys=metrics['history'], x_axis=\"_step\", pandas=(True), stream=\"default\")\n",
    "    history_metrics = {key: None for key in metrics['history']}\n",
    "    for key in history_metrics.keys():\n",
    "        try : \n",
    "            history_metrics[key] = history_metrics_full[key]\n",
    "        except:\n",
    "            print(f\"Skipping run {run.name} because it doesn't have the data {key} in history.\")\n",
    "            return None\n",
    "\n",
    "    ##### SUMMARY #####\n",
    "    summary_metrics = {key: None for key in metrics['summary']}\n",
    "    for key in summary_metrics.keys():\n",
    "        if key in run.summary:\n",
    "            summary_metrics[key] = run.summary._json_dict[key]\n",
    "        else:\n",
    "            summary_metrics[key] = history_metrics[key].iloc[-1]\n",
    "        \n",
    "    # Check env id \n",
    "    type_id = config_env[run.config.get('env_id')]['type_id']\n",
    "    return {\n",
    "        'exp_name': config_metrics['exp_name'],\n",
    "        'env_name': config_metrics['env_id'],\n",
    "        'type_id': type_id,\n",
    "        'seed': config_metrics['seed'],\n",
    "        'data': {\n",
    "            'summary_metrics': summary_metrics,\n",
    "            'history_metrics': history_metrics,\n",
    "            'config_metrics': config_metrics,\n",
    "            'config': config\n",
    "        }\n",
    "    }\n",
    "\n",
    "experiments_data = {}\n",
    "max_workers = 4\n",
    "# Utilisation de ThreadPoolExecutor pour paralléliser les exécutions de runs\n",
    "# Spécifiez le nombre de threads avec max_workers, par exemple 4 threads\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(process_run, run) for run in runs}\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing runs\"):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            exp_name = result['exp_name']\n",
    "            env_name = result['env_name']\n",
    "            type_id = result['type_id']\n",
    "            seed = result['seed']\n",
    "            data = result['data']\n",
    "            if exp_name not in experiments_data:\n",
    "                experiments_data[exp_name] = {}\n",
    "            if type_id not in experiments_data[exp_name]:\n",
    "                experiments_data[exp_name][type_id] = {}\n",
    "            if env_name not in experiments_data[exp_name][type_id]:\n",
    "                experiments_data[exp_name][type_id][env_name] = {}\n",
    "            if seed in experiments_data[exp_name][type_id][env_name].keys():\n",
    "                if experiments_data[exp_name][type_id][env_name][seed]['history_metrics']['charts/coverage'].max() < data['history_metrics']['charts/coverage'].max():\n",
    "                    experiments_data[exp_name][type_id][env_name][seed] = data\n",
    "            else:\n",
    "                experiments_data[exp_name][type_id][env_name][seed] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove v2wsac and v2klsac for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove experiences v2wsac \n",
    "del experiments_data['v2wsac']\n",
    "del experiments_data['v2klsac']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ngu_sac', 'metra_sac', 'diayn_sac', 'sac_vanilla', 'rnd_sac', 'icm_sac', 'smm_sac', 'lsd_sac', 'aux_sac', 'apt_sac', 'v1klsac', 'csd_sac', 'v1wsac']\n",
      "['maze', 'mujoco', 'robotics']\n",
      "['Maze-Hard-v0', 'Maze-Ur-v0', 'Maze-Easy-v0', 'Ant-v3', 'Walker2d-v3', 'Hopper-v3', 'Humanoid-v3', 'HalfCheetah-v3', 'FetchSlide-v2', 'FetchPush-v2', 'FetchReach-v1']\n"
     ]
    }
   ],
   "source": [
    "# algo\n",
    "list_algos = list(experiments_data.keys())\n",
    "# type\n",
    "list_type = list(experiments_data['v1klsac'].keys())\n",
    "# env\n",
    "list_env = []\n",
    "for i in range(len(list_type)):\n",
    "    list_env += list(list(experiments_data['v1klsac'][list_type[i]].keys()))\n",
    "\n",
    "print(list_algos)\n",
    "print(list_type)\n",
    "print(list_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traductor utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traductor_exp(exp_name:str):\n",
    "    # remove sac from name if it is present\n",
    "    exp_name = exp_name.replace('_sac', '')\n",
    "    # capitalize the full name\n",
    "    exp_name = exp_name.upper()\n",
    "    return exp_name \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table final coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key :  charts/coverage\n",
      "charts/coverage maze Normalized DataFrame:\n",
      "| exp_name    | Maze-Easy-v0   | Maze-Hard-v0    | Maze-Ur-v0     |\n",
      "|:------------|:---------------|:----------------|:---------------|\n",
      "| APT         | 98.39 +/- 0.31 | 77.54 +/- 0.86  | 89.47 +/- 1.8  |\n",
      "| AUX         | 26.52 +/- 0.32 | 26.96 +/- 0.2   | 26.53 +/- 0.16 |\n",
      "| CSD         | 76.74 +/- 2.09 | 50.14 +/- 1.42  | 57.16 +/- 1.33 |\n",
      "| DIAYN       | 54.5 +/- 2.86  | 40.5 +/- 1.93   | 45.37 +/- 6.42 |\n",
      "| ICM         | 83.83 +/- 2.82 | 66.45 +/- 11.48 | 67.15 +/- 6.87 |\n",
      "| LSD         | 74.93 +/- 2.13 | 46.01 +/- 1.99  | 52.24 +/- 2.46 |\n",
      "| METRA       | 53.47 +/- 4.99 | 38.71 +/- 5.74  | 51.09 +/- 3.57 |\n",
      "| NGU         | 58.95 +/- 1.88 | 42.32 +/- 4.41  | 44.01 +/- 5.36 |\n",
      "| RND         | 27.07 +/- 0.22 | 27.26 +/- 0.11  | 26.9 +/- 0.17  |\n",
      "| SAC_VANILLA | 27.41 +/- 0.23 | 27.65 +/- 0.25  | 27.07 +/- 0.28 |\n",
      "| SMM         | 56.3 +/- 3.17  | 42.94 +/- 1.25  | 48.15 +/- 5.53 |\n",
      "| V1KLSAC     | 98.51 +/- 0.44 | 71.23 +/- 4.08  | 79.56 +/- 2.24 |\n",
      "| V1WSAC      | 100.0 +/- 0.0  | 70.47 +/- 0.6   | 89.5 +/- 3.14  |\n",
      "Key :  charts/coverage\n",
      "charts/coverage mujoco Normalized DataFrame:\n",
      "| exp_name    | Ant-v3          | HalfCheetah-v3   | Hopper-v3       | Humanoid-v3    | Walker2d-v3     |\n",
      "|:------------|:----------------|:-----------------|:----------------|:---------------|:----------------|\n",
      "| APT         | 7.68 +/- 0.9    | 93.39 +/- 3.39   | 55.52 +/- 3.75  | 54.73 +/- 2.97 | 55.37 +/- 2.83  |\n",
      "| AUX         | 4.53 +/- 0.03   | 18.87 +/- 0.21   | 5.65 +/- 0.16   | 58.2 +/- 0.24  | 11.65 +/- 0.5   |\n",
      "| CSD         | 2.37 +/- 0.25   | 15.51 +/- 1.93   | 5.85 +/- 0.3    | 74.93 +/- 6.87 | 7.31 +/- 0.44   |\n",
      "| DIAYN       | 11.76 +/- 0.61  | 58.18 +/- 5.65   | 15.03 +/- 8.89  | 70.68 +/- 4.11 | 14.84 +/- 1.34  |\n",
      "| ICM         | 3.26 +/- 0.13   | 28.9 +/- 1.55    | 41.63 +/- 0.56  | 58.96 +/- 0.87 | 33.81 +/- 1.95  |\n",
      "| LSD         | 7.01 +/- 2.15   | 30.43 +/- 2.79   | 18.38 +/- 5.27  | 69.89 +/- 2.25 | 17.26 +/- 2.32  |\n",
      "| METRA       | 23.46 +/- 0.74  | 73.82 +/- 3.0    | 37.5 +/- 3.33   | 88.35 +/- 5.05 | 36.88 +/- 4.18  |\n",
      "| NGU         | 2.79 +/- 0.07   | 25.53 +/- 0.42   | 19.55 +/- 0.62  | 44.02 +/- 4.3  | 27.26 +/- 2.01  |\n",
      "| RND         | 4.57 +/- 0.07   | 19.1 +/- 0.14    | 6.95 +/- 0.51   | 57.91 +/- 0.2  | 13.19 +/- 0.31  |\n",
      "| SAC_VANILLA | 4.4 +/- 0.05    | 18.42 +/- 0.24   | 5.83 +/- 0.15   | 57.94 +/- 0.27 | 13.11 +/- 0.92  |\n",
      "| SMM         | 10.61 +/- 1.28  | 58.91 +/- 5.2    | 43.41 +/- 14.93 | 32.64 +/- 3.09 | 44.21 +/- 13.19 |\n",
      "| V1KLSAC     | 1.2 +/- 0.05    | 29.76 +/- 1.12   | 8.6 +/- 0.55    | 30.53 +/- 1.54 | 17.52 +/- 2.04  |\n",
      "| V1WSAC      | 78.35 +/- 13.45 | 40.33 +/- 6.69   | 74.43 +/- 12.14 | 90.5 +/- 2.29  | 74.89 +/- 11.71 |\n",
      "Key :  charts/coverage\n",
      "charts/coverage robotics Normalized DataFrame:\n",
      "| exp_name    | FetchPush-v2   | FetchReach-v1   | FetchSlide-v2   |\n",
      "|:------------|:---------------|:----------------|:----------------|\n",
      "| APT         | 90.87 +/- 3.59 | 59.37 +/- 1.92  | 95.43 +/- 4.5   |\n",
      "| AUX         | 20.27 +/- 0.91 | 19.5 +/- 0.09   | 46.59 +/- 2.55  |\n",
      "| CSD         | 11.02 +/- 0.5  | 38.26 +/- 1.46  | 20.47 +/- 0.68  |\n",
      "| DIAYN       | 18.12 +/- 0.54 | 33.98 +/- 2.24  | 45.05 +/- 3.68  |\n",
      "| ICM         | 36.32 +/- 4.24 | 77.51 +/- 1.16  | 42.68 +/- 2.21  |\n",
      "| LSD         | 36.24 +/- 6.01 | 98.08 +/- 0.72  | 47.49 +/- 1.75  |\n",
      "| METRA       | 10.99 +/- 0.43 | 68.05 +/- 1.38  | 17.01 +/- 1.47  |\n",
      "| NGU         | 23.85 +/- 0.94 | 62.22 +/- 1.22  | 44.91 +/- 1.99  |\n",
      "| RND         | 21.86 +/- 0.32 | 19.49 +/- 0.16  | 54.66 +/- 2.57  |\n",
      "| SAC_VANILLA | 21.24 +/- 0.73 | 19.41 +/- 0.1   | 47.14 +/- 2.7   |\n",
      "| SMM         | 69.06 +/- 5.98 | 29.37 +/- 2.29  | 64.96 +/- 1.26  |\n",
      "| V1KLSAC     | 23.61 +/- 3.15 | 58.57 +/- 2.5   | 35.55 +/- 2.83  |\n",
      "| V1WSAC      | 34.2 +/- 6.18  | 81.56 +/- 1.42  | 51.11 +/- 2.9   |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "def extract_final_values(experiments_data, \n",
    "                        keys=['charts/coverage'],\n",
    "                        type_id_default = None, \n",
    "                        env_name_default = None):\n",
    "    key_data = {}\n",
    "    for exp_name in experiments_data.keys():\n",
    "        for type_id in experiments_data[exp_name].keys():\n",
    "            if type_id_default is not None and type_id not in type_id_default:\n",
    "                continue\n",
    "            for env_name in experiments_data[exp_name][type_id].keys():\n",
    "                if env_name_default is not None and env_name not in env_name_default:\n",
    "                    continue\n",
    "                for seed in experiments_data[exp_name][type_id][env_name].keys():\n",
    "                    run_data = experiments_data[exp_name][type_id][env_name][seed]\n",
    "                    for key in keys:\n",
    "                        if key not in key_data:\n",
    "                            key_data[key] = []\n",
    "                        metric = run_data['summary_metrics'][key]\n",
    "                        key_data[key].append({\n",
    "                            'exp_name': traductor_exp(exp_name),\n",
    "                            'env_name': env_name,\n",
    "                            'seed': seed,\n",
    "                            key: metric\n",
    "                        })\n",
    "    key_df = {}\n",
    "    for key in keys:\n",
    "        key_df[key] = pd.DataFrame(key_data[key])\n",
    "    return key_df \n",
    "\n",
    "# Fonction pour normaliser le coverage par le coverage maximal de l'environnement\n",
    "def normalize(df, key):\n",
    "    normalized_df = df[key].copy()\n",
    "    print(\"Key : \", key)\n",
    "    normalized_df[key+'_max'] = normalized_df.groupby(['env_name'])[key].transform('max')\n",
    "    normalized_df[key+'_mean'] = normalized_df.groupby(['exp_name', 'env_name'])[key].transform('mean')\n",
    "    normalized_df[key+'_std'] = normalized_df.groupby(['exp_name', 'env_name'])[key].transform('std')\n",
    "    normalized_df[key+'_ste'] = normalized_df.groupby(['exp_name', 'env_name'])[key].transform('sem')\n",
    "    \n",
    "    normalized_df[key+'_normalized_mean'] = (normalized_df[key+'_mean'] / np.abs(normalized_df[key+'_max'])) * 100\n",
    "    normalized_df[key+'_normalized_std'] = (normalized_df[key+'_std'] / np.abs(normalized_df[key+'_max'])) * 100\n",
    "    normalized_df[key+'_normalized_ste'] = (normalized_df[key+'_ste'] / np.abs(normalized_df[key+'_max'])) * 100\n",
    "\n",
    "    # normalized_df[key+'_normalized_mean'] = normalized_df[key+'_mean'] \n",
    "    # normalized_df[key+'_normalized_std'] = normalized_df[key+'_std'] \n",
    "    # normalized_df[key+'_normalized_ste'] = normalized_df[key+'_ste'] \n",
    "\n",
    "    \n",
    "    normalized_df = normalized_df[['exp_name', 'env_name', key+'_normalized_mean', key+'_normalized_std', key+'_normalized_ste']].drop_duplicates()\n",
    "    return normalized_df\n",
    "\n",
    "\n",
    "def format_results(df, value_col_mean, value_col_std):\n",
    "    formatted_results = df.pivot(index='exp_name', columns='env_name', values=[value_col_mean, value_col_std])\n",
    "    formatted_results = formatted_results.swaplevel(axis=1).sort_index(axis=1, level=0)\n",
    "    for env in formatted_results.columns.levels[0]:\n",
    "        formatted_results[(env, 'mean +/- std')] = formatted_results[(env, value_col_mean)].round(2).astype(str) + \" +/- \" + formatted_results[(env, value_col_std)].round(2).astype(str)\n",
    "    formatted_results = formatted_results.loc[:, pd.IndexSlice[:, 'mean +/- std']]\n",
    "    formatted_results.columns = formatted_results.columns.droplevel(1)\n",
    "    return formatted_results\n",
    "\n",
    "# def dataframe_to_markdown(df, filename):\n",
    "#     # Apply bold formatting to each row\n",
    "#     # df_bold = df.apply(bold_max_in_row, axis=1)\n",
    "#     # Convert DataFrame to Markdown table format\n",
    "#     markdown_table = tabulate(df, headers='keys', tablefmt='pipe', showindex=True)\n",
    "#     # Save the Markdown table to a file\n",
    "#     filename = filename.replace('/', '_')\n",
    "#     with open(filename, 'w') as f:\n",
    "#         f.write(markdown_table)\n",
    "\n",
    "def dataframe_to_markdown(df, filename):\n",
    "    def bold_max_in_column(df):\n",
    "        df_bold = df.copy()\n",
    "        for col in df.columns:\n",
    "            if col != 'exp_name':\n",
    "                max_value = df[col].apply(lambda x: float(x.split(' +/- ')[0])).max()\n",
    "                df_bold[col] = df[col].apply(lambda x: f\"**{x}**\" if float(x.split(' +/- ')[0]) == max_value else x)\n",
    "        return df_bold\n",
    "    \n",
    "    # Apply bold formatting to each column\n",
    "    df_bold = bold_max_in_column(df)\n",
    "    \n",
    "    # Convert DataFrame to Markdown format\n",
    "    markdown_table = tabulate(df_bold, headers='keys', tablefmt='pipe', showindex=True)\n",
    "    \n",
    "    # Replace '/' in filename\n",
    "    filename = filename.replace('/', '_')\n",
    "    \n",
    "    # Write the Markdown table to a file\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(markdown_table)\n",
    "\n",
    "def dataframe_to_latex(df, filename='latex_table.txt', save=False):\n",
    "    def bold_max_in_column(df):\n",
    "        df_bold = df.copy()\n",
    "        for col in df.columns:\n",
    "            if col != 'exp_name':\n",
    "                max_value = df[col].apply(lambda x: float(x.split(' +/- ')[0])).max()\n",
    "                df_bold[col] = df[col].apply(lambda x: f\"\\\\textbf{{{x}}}\" if float(x.split(' +/- ')[0]) == max_value else x)\n",
    "        return df_bold\n",
    "    \n",
    "    # Apply bold formatting to each column\n",
    "    df_bold = bold_max_in_column(df)\n",
    "    \n",
    "    # Convert DataFrame to LaTeX format\n",
    "    latex_table = tabulate(df_bold, headers='keys', tablefmt='latex', showindex=True)\n",
    "    \n",
    "    # Correct the LaTeX syntax\n",
    "    latex_table = latex_table.replace(r'\\textbackslash{}', '\\\\').replace(r'\\\\textbf', r'\\textbf').replace(r'\\{', '{').replace(r'\\}', '}')\n",
    "    \n",
    "    # Replace '/' in filename\n",
    "    filename = filename.replace('/', '_')\n",
    "    \n",
    "    if save:\n",
    "        # Delete file if it already exists\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(latex_table)\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "\n",
    "# Extraction des valeurs finales\n",
    "for type_id in list_type:\n",
    "    key_df = extract_final_values(experiments_data, type_id_default=[type_id], env_name_default=list(experiments_data['v1klsac'][type_id].keys()))\n",
    "    for key in key_df.keys():\n",
    "        # coverage normalization \n",
    "        data_normalized_df = normalize(key_df, key)\n",
    "        # # Formater les résultats\n",
    "        data_formatted = format_results(data_normalized_df, key+'_normalized_mean', key+'_normalized_ste')\n",
    "        # # # Enregistrement des DataFrames au format Markdown\n",
    "        # # dataframe_to_markdown(coverage_formatted, 'coverage_normalized.md')\n",
    "        # dataframe_to_markdown(data_formatted, 'shannon_entropy.md')\n",
    "\n",
    "        # # Enregistrement des DataFrames au format LaTeX\n",
    "        dataframe_to_latex(data_formatted, f'{key}_{type_id}_normalized.tex', save=True if 'mu' not in key else False)\n",
    "\n",
    "        # mise en forme en markdown\n",
    "        # dataframe_to_markdown(data_formatted, f'{key}_{type_id}_normalized.md')\n",
    "        # Affichage des résultats\n",
    "        print(key + \" \" + type_id +\" Normalized DataFrame:\")\n",
    "        print(tabulate(data_formatted, headers='keys', tablefmt='pipe', showindex=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fonction pour vérifier si une liste est vide\n",
    "def is_not_empty(obj):\n",
    "    if isinstance(obj, pd.Series):\n",
    "        return not obj.empty\n",
    "    return bool(obj)\n",
    "\n",
    "def interpolate_and_fill_single_metric(values, global_steps, max_global_step):\n",
    "    values = values.dropna()\n",
    "    # Créer une série avec des NaNs pour les global_steps manquants\n",
    "    all_steps = np.arange(max_global_step + 1)\n",
    "    series = values.reindex(all_steps).interpolate(method='linear').ffill().bfill()\n",
    "    return series\n",
    "\n",
    "def add_nomalized_data(experiments_data, keys=['charts/coverage', '_step']):\n",
    "    # final data \n",
    "    data = {}\n",
    "    # max data evaluation \n",
    "    dict_max = {}\n",
    "    for exp_name in experiments_data.keys():\n",
    "        for type_id in experiments_data[exp_name].keys():\n",
    "            for env_name in experiments_data[exp_name][type_id].keys():\n",
    "                dict_max[env_name] = {} if env_name not in dict_max else dict_max[env_name]\n",
    "                for seed in experiments_data[exp_name][type_id][env_name].keys():\n",
    "                    for key in keys:\n",
    "                        if key not in dict_max[env_name]:\n",
    "                            dict_max[env_name][key] = experiments_data[exp_name][type_id][env_name][seed]['summary_metrics'][key]\n",
    "                        else : \n",
    "                            dict_max[env_name][key] = max(dict_max[env_name][key], experiments_data[exp_name][type_id][env_name][seed]['summary_metrics'][key])\n",
    "                        # nb_values\n",
    "                        if \"nb_values\" not in dict_max[env_name]: dict_max[env_name][\"nb_values\"] = experiments_data[exp_name][type_id][env_name][seed]['history_metrics'][key].shape[0]  \n",
    "                        else : dict_max[env_name][\"nb_values\"] = max(experiments_data[exp_name][type_id][env_name][seed]['history_metrics'][key].shape[0], dict_max[env_name][\"nb_values\"])\n",
    "    # add normalized data\n",
    "    for exp_name in experiments_data.keys():\n",
    "        data[exp_name] = {} if exp_name not in data else data[exp_name]\n",
    "        for type_id in experiments_data[exp_name].keys():\n",
    "            data[exp_name][type_id] = {} if type_id not in data[exp_name] else data[exp_name][type_id]\n",
    "            for env_name in experiments_data[exp_name][type_id].keys():\n",
    "                data[exp_name][type_id][env_name] = {} if env_name not in data[exp_name][type_id] else data[exp_name][type_id][env_name]\n",
    "                for key in keys:\n",
    "                    if key == '_step':\n",
    "                        continue\n",
    "                    data_seeds = []\n",
    "                    for seed in experiments_data[exp_name][type_id][env_name].keys():\n",
    "                        run_data = experiments_data[exp_name][type_id][env_name][seed]\n",
    "                        values = run_data['history_metrics'][key]\n",
    "                        # repeat last value nb_values-values times\n",
    "                        df_comp = pd.Series(np.repeat(values.iloc[-1], dict_max[env_name][\"nb_values\"]-values.shape[0]))\n",
    "                        values = pd.concat([values, df_comp]).values.reshape(-1, 1)\n",
    "                        data_seeds.append(pd.Series(values.flatten()))\n",
    "                \n",
    "                    df_concat = pd.concat(data_seeds, axis=1)\n",
    "                    df_mean = (df_concat.mean(axis=1)/dict_max[env_name][key])*100.0\n",
    "                    df_std = (df_concat.std(axis=1)/dict_max[env_name][key])*100.0\n",
    "                    df_ste = (df_concat.sem(axis=1)/dict_max[env_name][key])*100.0\n",
    "                    # df_mean = (df_concat.mean(axis=1)/dict_max[env_name][key])*100.0\n",
    "                    # df_std = (df_concat.std(axis=1)/dict_max[env_name][key])*100.0\n",
    "                    # df_ste = (df_concat.sem(axis=1)/dict_max[env_name][key])*100.0\n",
    "                    data[exp_name][type_id][env_name]['normalized_mean_'+key] = df_mean\n",
    "                    data[exp_name][type_id][env_name]['normalized_std_'+key] = df_std\n",
    "                    data[exp_name][type_id][env_name]['normalized_ste_'+key] = df_ste\n",
    "                data[exp_name][type_id][env_name]['_step'] = run_data['history_metrics']['_step']\n",
    "                data[exp_name][type_id][env_name]['max_step'] = dict_max[env_name][\"_step\"]\n",
    "    return data\n",
    "                    \n",
    "                        \n",
    "        \n",
    "data_clean = add_nomalized_data(experiments_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rnd_sac', 'sac_vanilla', 'icm_sac', 'ngu_sac', 'diayn_sac', 'smm_sac', 'lsd_sac', 'aux_sac', 'metra_sac', 'apt_sac', 'v1klsac', 'csd_sac', 'v1wsac'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Déterminer les couleurs pour chaque algorithme\n",
    "palette = plt.get_cmap(\"tab20\").colors\n",
    "color_map = {exp_name: palette[i % len(palette)] for i, exp_name in enumerate(data_clean.keys())}\n",
    "key_plot = ['charts/coverage']\n",
    "for type_id in data_clean['v1klsac'].keys():\n",
    "    for env in data_clean['v1klsac'][type_id].keys():\n",
    "        for data_key in key_plot : \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            # try : \n",
    "            for exp_name in data_clean.keys():\n",
    "                if type_id in data_clean[exp_name] and env in data_clean[exp_name][type_id]:\n",
    "                    # if exp_name != algo:\n",
    "                    #     continue\n",
    "                    # if data_key not in data[type_id][env]:\n",
    "                    #     continue\n",
    "                    # x = data[type_id][env]['_step']\n",
    "                    if type_id=='mujoco' :\n",
    "                        x= np.linspace(0, TOTAL_TIME_STEPS*NUM_ENVS,data_clean[exp_name][type_id][env]['normalized_mean_'+data_key].shape[0])\n",
    "                    else:\n",
    "                        x = np.linspace(0, data_clean[exp_name][type_id][env]['max_step']*NUM_ENVS, data_clean[exp_name][type_id][env]['normalized_mean_'+data_key].shape[0])\n",
    "                    mean = data_clean[exp_name][type_id][env]['normalized_mean_'+data_key][:len(x)] \n",
    "                    std = data_clean[exp_name][type_id][env]['normalized_std_'+data_key][:len(x)]               \n",
    "                    ste = data_clean[exp_name][type_id][env]['normalized_ste_'+data_key][:len(x)]\n",
    "                    plt.plot(x, mean, label=traductor_exp(exp_name), color=color_map[exp_name])\n",
    "                    plt.fill_between(x, mean - ste, mean + ste, color=color_map[exp_name], alpha=0.2)\n",
    "            plt.title(f\"{data_key} - {env} - {type_id}\")\n",
    "            plt.xlabel(\"Steps\")\n",
    "            plt.ylabel(\"Normalized value (%)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            # except Exception as e:\n",
    "            #     print(e)\n",
    "            #     print(f\"Error in {exp_name} - {env} - {type_id} - {data_key}\")\n",
    "            #     continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test maze plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_data['v2klsac']['maze']['Maze-Hard-v0'].keys()\n",
    "# concat df covergae \n",
    "df_coverage = pd.concat([experiments_data['v2klsac']['maze']['Maze-Hard-v0'][seed]['history_metrics']['charts/coverage'] for seed in experiments_data['v2klsac']['maze']['Maze-Hard-v0'].keys()], axis=1)\n",
    "# mean coverage on dim 1\n",
    "df_coverage_mean = df_coverage.mean(axis=1)\n",
    "# standard error coverage on dim 1\n",
    "df_coverage_ste = df_coverage.std(axis=1) / np.sqrt(df_coverage.shape[1])\n",
    "\n",
    "# figure \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df_coverage_mean, label='mean')\n",
    "plt.fill_between(df_coverage_mean.index, df_coverage_mean - df_coverage_ste, df_coverage_mean + df_coverage_ste, alpha=0.2)\n",
    "plt.title('Maze-Hard-v0 Coverage')  \n",
    "plt.xlabel('Global Step')\n",
    "plt.ylabel('Coverage')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
